---
title: "Project 4"
author: "J. Hamski"
date: "April 22, 2015"
output: html_document
---

```{r, warning=FALSE, message=FALSE}
library(rvest)
library(dplyr)
```

##Project Fundamentals: Scraping Title, Publication Date, and Author  
For each of the reference blog entries on the first page, you should pull out the title, date, and author, and store these in an R data frame.

```{r}
search.results <-html("http://www.r-bloggers.com/search/web%20scraping")

titles <- search.results %>%
  html_nodes("#leftcontent h2") %>%
  html_text()

dates <- search.results %>%
  html_nodes(".date") %>%
  html_text()

authors <- search.results %>%
  html_nodes("div.meta > a") %>%
  html_text()
 
rBloggers <- as.data.frame(cbind(titles, dates, authors))
str(rBloggers)
```

##Next Step: Scraping All 17 Result Pages  

```{r}

scrape.results <-function(URL){
  
  search.results <- html(URL)
  
  titles <- search.results %>%
  html_nodes("#leftcontent h2") %>%
  html_text()

dates <- search.results %>%
  html_nodes(".date") %>%
  html_text()

authors <- search.results %>%
  html_nodes("div.meta > a") %>%
  html_text()
  
  rBloggers <- as.data.frame(cbind(titles, dates, authors))
  
  Sys.sleep(1)
  
  return(rBloggers)
}

construct.URL <- function(page){
  url.part <-("http://www.r-bloggers.com/search/web%20scraping/page/") #5/
  URL <- paste(url.part,page, sep="")
  return(URL)
}
 
pages <- 1:17

URLs <- sapply(pages, FUN = construct.URL)

scrape.all <- lapply(URLs, FUN = scrape.results)
scrape.all <- do.call("rbind", scrape.all)

str(scrape.all)

```

##Final Step: Wordcloud 

```{r, warning=FALSE, message=FALSE}
library(tm)
library(wordcloud)

words <- Corpus(VectorSource(scrape.all$titles))

#wordcloud(words)

#based on: https://georeferenced.wordpress.com/2013/01/15/rwordcloud/

library(SnowballC)

words.proc <- words %>%
  tm_map(stripWhitespace)%>%
  tm_map(content_transformer(tolower))%>%
  tm_map(removeWords, stopwords("english"))%>%
  tm_map(stemDocument)

wordcloud(words.proc)
```

```{r, warning=FALSE, message=FALSE}

```


